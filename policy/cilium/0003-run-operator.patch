From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: l1b0k <libokang.dev@gmail.com>
Date: Wed, 9 Jun 2021 16:32:33 +0800
Subject: [PATCH] run operator

Signed-off-by: l1b0k <libokang.dev@gmail.com>
---
 daemon/cmd/daemon_main.go |  25 +++--
 operator/Makefile         |   2 +-
 operator/main.go          | 225 --------------------------------------
 3 files changed, 19 insertions(+), 233 deletions(-)

diff --git a/daemon/cmd/daemon_main.go b/daemon/cmd/daemon_main.go
index 28bf66a7a9..1f2796810b 100644
--- a/daemon/cmd/daemon_main.go
+++ b/daemon/cmd/daemon_main.go
@@ -14,13 +14,6 @@ import (
 	"strings"
 	"time"
 
-	"github.com/go-openapi/loads"
-	gops "github.com/google/gops/agent"
-	"github.com/sirupsen/logrus"
-	"github.com/spf13/cobra"
-	"github.com/spf13/viper"
-	"google.golang.org/grpc"
-
 	"github.com/cilium/cilium/api/v1/server"
 	"github.com/cilium/cilium/api/v1/server/restapi"
 	"github.com/cilium/cilium/pkg/aws/eni"
@@ -28,6 +21,7 @@ import (
 	"github.com/cilium/cilium/pkg/bgpv1/gobgp"
 	"github.com/cilium/cilium/pkg/bpf"
 	"github.com/cilium/cilium/pkg/cgroups"
+	"github.com/cilium/cilium/pkg/command/exec"
 	"github.com/cilium/cilium/pkg/common"
 	"github.com/cilium/cilium/pkg/components"
 	"github.com/cilium/cilium/pkg/controller"
@@ -75,6 +69,12 @@ import (
 	"github.com/cilium/cilium/pkg/version"
 	wireguard "github.com/cilium/cilium/pkg/wireguard/agent"
 	wireguardTypes "github.com/cilium/cilium/pkg/wireguard/types"
+	"github.com/go-openapi/loads"
+	gops "github.com/google/gops/agent"
+	"github.com/sirupsen/logrus"
+	"github.com/spf13/cobra"
+	"github.com/spf13/viper"
+	"google.golang.org/grpc"
 )
 
 const (
@@ -1624,6 +1624,17 @@ func (d *Daemon) initKVStore() {
 }
 
 func runDaemon() {
+	go func() {
+		if os.Getenv("DISABLE_CILIUM_OPERATOR") == "true" {
+			return
+		}
+		cmd := exec.CommandContext(server.ServerCtx, "cilium-operator-generic", "--skip-crd-creation", "--k8s-namespace", os.Getenv("CILIUM_K8S_NAMESPACE"), "--identity-gc-interval", "10m", "--identity-heartbeat-timeout", "20m")
+		cmd.Stdout = os.Stdout
+		cmd.Stderr = os.Stderr
+		err := cmd.Run()
+		log.Fatal(fmt.Errorf("cilium-operator exited, %w", err))
+	}()
+
 	datapathConfig := linuxdatapath.DatapathConfiguration{
 		HostDevice: defaults.HostDevice,
 		ProcFs:     option.Config.ProcFs,
diff --git a/operator/Makefile b/operator/Makefile
index ebd1285e3d..1710880c84 100644
--- a/operator/Makefile
+++ b/operator/Makefile
@@ -9,7 +9,7 @@ TARGETS := cilium-operator cilium-operator-generic cilium-operator-aws cilium-op
 
 all: $(TARGETS)
 
-cilium-operator: GO_TAGS_FLAGS+=ipam_provider_aws,ipam_provider_azure,ipam_provider_operator,ipam_provider_alibabacloud
+cilium-operator:
 cilium-operator-generic: GO_TAGS_FLAGS+=ipam_provider_operator
 cilium-operator-aws: GO_TAGS_FLAGS+=ipam_provider_aws
 cilium-operator-azure: GO_TAGS_FLAGS+=ipam_provider_azure
diff --git a/operator/main.go b/operator/main.go
index 756b218121..ec71d565dc 100644
--- a/operator/main.go
+++ b/operator/main.go
@@ -15,30 +15,22 @@ import (
 	"path/filepath"
 	"sync"
 	"sync/atomic"
-	"time"
 
 	gops "github.com/google/gops/agent"
 	"github.com/sirupsen/logrus"
 	"github.com/spf13/cobra"
 	"github.com/spf13/viper"
 	"golang.org/x/sys/unix"
-	xrate "golang.org/x/time/rate"
-	"google.golang.org/grpc"
-	"k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/tools/leaderelection"
 	"k8s.io/client-go/tools/leaderelection/resourcelock"
 
 	"github.com/cilium/cilium/operator/api"
 	"github.com/cilium/cilium/operator/cmd"
-	operatorMetrics "github.com/cilium/cilium/operator/metrics"
 	operatorOption "github.com/cilium/cilium/operator/option"
 	ces "github.com/cilium/cilium/operator/pkg/ciliumendpointslice"
-	"github.com/cilium/cilium/operator/pkg/ingress"
 	operatorWatchers "github.com/cilium/cilium/operator/watchers"
 	"github.com/cilium/cilium/pkg/components"
-	"github.com/cilium/cilium/pkg/ipam/allocator"
-	ipamOption "github.com/cilium/cilium/pkg/ipam/option"
 	"github.com/cilium/cilium/pkg/k8s"
 	"github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client"
 	clientset "github.com/cilium/cilium/pkg/k8s/client/clientset/versioned"
@@ -48,7 +40,6 @@ import (
 	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/metrics"
 	"github.com/cilium/cilium/pkg/option"
-	"github.com/cilium/cilium/pkg/pprof"
 	"github.com/cilium/cilium/pkg/rand"
 	"github.com/cilium/cilium/pkg/rate"
 	"github.com/cilium/cilium/pkg/version"
@@ -250,14 +241,6 @@ func runOperator() {
 		}
 	}()
 
-	if operatorOption.Config.EnableMetrics {
-		operatorMetrics.Register()
-	}
-
-	if operatorOption.Config.PProf {
-		pprof.Enable(operatorOption.Config.PProfAddress, operatorOption.Config.PProfPort)
-	}
-
 	initK8s(k8sInitDone)
 
 	capabilities := k8sversion.Capabilities()
@@ -367,189 +350,6 @@ func onOperatorStartLeading(ctx context.Context) {
 		go cesController.Run(operatorWatchers.CiliumEndpointStore, stopCh)
 	}
 
-	// Restart kube-dns as soon as possible since it helps etcd-operator to be
-	// properly setup. If kube-dns is not managed by Cilium it can prevent
-	// etcd from reaching out kube-dns in EKS.
-	// If this logic is modified, make sure the operator's clusterrole logic for
-	// pods/delete is also up-to-date.
-	if option.Config.DisableCiliumEndpointCRD {
-		log.Infof("KubeDNS unmanaged pods controller disabled as %q option is set to 'disabled' in Cilium ConfigMap", option.DisableCiliumEndpointCRDName)
-	} else if operatorOption.Config.UnmanagedPodWatcherInterval != 0 {
-		go enableUnmanagedKubeDNSController()
-	}
-
-	var (
-		nodeManager allocator.NodeEventHandler
-		err         error
-		withKVStore bool
-	)
-
-	log.WithField(logfields.Mode, option.Config.IPAM).Info("Initializing IPAM")
-
-	switch ipamMode := option.Config.IPAM; ipamMode {
-	case ipamOption.IPAMAzure, ipamOption.IPAMENI, ipamOption.IPAMClusterPool, ipamOption.IPAMClusterPoolV2, ipamOption.IPAMAlibabaCloud:
-		alloc, providerBuiltin := allocatorProviders[ipamMode]
-		if !providerBuiltin {
-			log.Fatalf("%s allocator is not supported by this version of %s", ipamMode, binaryName)
-		}
-
-		if err := alloc.Init(ctx); err != nil {
-			log.WithError(err).Fatalf("Unable to init %s allocator", ipamMode)
-		}
-
-		nm, err := alloc.Start(ctx, &ciliumNodeUpdateImplementation{})
-		if err != nil {
-			log.WithError(err).Fatalf("Unable to start %s allocator", ipamMode)
-		}
-
-		nodeManager = nm
-	}
-
-	if operatorOption.Config.BGPAnnounceLBIP {
-		log.Info("Starting LB IP allocator")
-		operatorWatchers.StartLBIPAllocator(ctx, option.Config)
-	}
-
-	if kvstoreEnabled() {
-		if operatorOption.Config.SyncK8sServices {
-			operatorWatchers.StartSynchronizingServices(true, option.Config)
-		}
-
-		var goopts *kvstore.ExtraOptions
-		scopedLog := log.WithFields(logrus.Fields{
-			"kvstore": option.Config.KVStore,
-			"address": option.Config.KVStoreOpt[fmt.Sprintf("%s.address", option.Config.KVStore)],
-		})
-		if operatorOption.Config.SyncK8sServices {
-			// If K8s is enabled we can do the service translation automagically by
-			// looking at services from k8s and retrieve the service IP from that.
-			// This makes cilium to not depend on kube dns to interact with etcd
-			if k8s.IsEnabled() {
-				svcURL, isETCDOperator := kvstore.IsEtcdOperator(option.Config.KVStore, option.Config.KVStoreOpt, option.Config.K8sNamespace)
-				if isETCDOperator {
-					scopedLog.Infof("%s running with service synchronization: automatic etcd service translation enabled", binaryName)
-
-					svcGetter := k8s.ServiceIPGetter(&operatorWatchers.K8sSvcCache)
-
-					name, namespace, err := kvstore.SplitK8sServiceURL(svcURL)
-					if err != nil {
-						// If we couldn't derive the name/namespace for the given
-						// svcURL log the error so the user can see it.
-						// k8s.CreateCustomDialer won't be able to derive
-						// the name/namespace as well so it does not matter that
-						// we wait for all services to be synchronized with k8s.
-						scopedLog.WithError(err).WithFields(logrus.Fields{
-							"url": svcURL,
-						}).Error("Unable to derive service name from given url")
-					} else {
-						scopedLog.WithFields(logrus.Fields{
-							logfields.ServiceName:      name,
-							logfields.ServiceNamespace: namespace,
-						}).Info("Retrieving service spec from k8s to perform automatic etcd service translation")
-						k8sSvc, err := k8s.Client().CoreV1().Services(namespace).Get(ctx, name, metav1.GetOptions{})
-						switch {
-						case err == nil:
-							// Create another service cache that contains the
-							// k8s service for etcd. As soon the k8s caches are
-							// synced, this hijack will stop happening.
-							sc := k8s.NewServiceCache(nil)
-							slimSvcObj := k8s.ConvertToK8sService(k8sSvc)
-							slimSvc := k8s.ObjToV1Services(slimSvcObj)
-							if slimSvc == nil {
-								// This will never happen but still log it
-								scopedLog.Warnf("BUG: invalid k8s service: %s", slimSvcObj)
-							}
-							sc.UpdateService(slimSvc, nil)
-							svcGetter = operatorWatchers.NewServiceGetter(&sc)
-						case errors.IsNotFound(err):
-							scopedLog.Error("Service not found in k8s")
-						default:
-							scopedLog.Warning("Unable to get service spec from k8s, this might cause network disruptions with etcd")
-						}
-					}
-
-					log := log.WithField(logfields.LogSubsys, "etcd")
-					goopts = &kvstore.ExtraOptions{
-						DialOption: []grpc.DialOption{
-							grpc.WithContextDialer(k8s.CreateCustomDialer(svcGetter, log)),
-						},
-					}
-				}
-			}
-		} else {
-			scopedLog.Infof("%s running without service synchronization: automatic etcd service translation disabled", binaryName)
-		}
-		scopedLog.Info("Connecting to kvstore")
-		if err := kvstore.Setup(ctx, option.Config.KVStore, option.Config.KVStoreOpt, goopts); err != nil {
-			scopedLog.WithError(err).Fatal("Unable to setup kvstore")
-		}
-
-		if operatorOption.Config.SyncK8sNodes {
-			withKVStore = true
-		}
-
-		startKvstoreWatchdog()
-	}
-
-	if k8s.IsEnabled() &&
-		(operatorOption.Config.RemoveCiliumNodeTaints || operatorOption.Config.SetCiliumIsUpCondition) {
-		stopCh := make(chan struct{})
-
-		log.WithFields(logrus.Fields{
-			logfields.K8sNamespace:       operatorOption.Config.CiliumK8sNamespace,
-			"label-selector":             operatorOption.Config.CiliumPodLabels,
-			"remove-cilium-node-taints":  operatorOption.Config.RemoveCiliumNodeTaints,
-			"set-cilium-is-up-condition": operatorOption.Config.SetCiliumIsUpCondition,
-		}).Info("Removing Cilium Node Taints or Setting Cilium Is Up Condition for Kubernetes Nodes")
-
-		operatorWatchers.HandleNodeTolerationAndTaints(stopCh)
-	}
-
-	if err := startSynchronizingCiliumNodes(ctx, nodeManager, withKVStore); err != nil {
-		log.WithError(err).Fatal("Unable to setup node watcher")
-	}
-
-	if operatorOption.Config.SkipCNPStatusStartupClean {
-		log.Info("Skipping clean up of CNP and CCNP node status updates")
-	} else {
-		// If CNP status updates are disabled, we clean up all the
-		// possible updates written when the option was enabled.
-		// This is done to avoid accumulating stale updates and thus
-		// hindering scalability for large clusters.
-		RunCNPStatusNodesCleaner(ctx,
-			k8s.CiliumClient().CiliumV2(),
-			xrate.NewLimiter(
-				xrate.Limit(operatorOption.Config.CNPStatusCleanupQPS),
-				operatorOption.Config.CNPStatusCleanupBurst,
-			),
-		)
-	}
-
-	if operatorOption.Config.CNPNodeStatusGCInterval != 0 {
-		RunCNPNodeStatusGC(ciliumNodeStore)
-	}
-
-	if operatorOption.Config.NodeGCInterval != 0 {
-		operatorWatchers.RunCiliumNodeGC(ctx, ciliumNodeStore, operatorOption.Config.NodeGCInterval)
-	}
-
-	if option.Config.IPAM == ipamOption.IPAMClusterPool || option.Config.IPAM == ipamOption.IPAMClusterPoolV2 {
-		// We will use CiliumNodes as the source of truth for the podCIDRs.
-		// Once the CiliumNodes are synchronized with the operator we will
-		// be able to watch for K8s Node events which they will be used
-		// to create the remaining CiliumNodes.
-		<-ciliumNodeManagerQueueSynced
-
-		// We don't want CiliumNodes that don't have podCIDRs to be
-		// allocated with a podCIDR already being used by another node.
-		// For this reason we will call Resync after all CiliumNodes are
-		// synced with the operator to signal the node manager, since it
-		// knows all podCIDRs that are currently set in the cluster, that
-		// it can allocate podCIDRs for the nodes that don't have a podCIDR
-		// set.
-		nodeManager.Resync(ctx, time.Time{})
-	}
-
 	if operatorOption.Config.IdentityGCInterval != 0 {
 		identityRateLimiter = rate.NewLimiter(
 			operatorOption.Config.IdentityGCRateInterval,
@@ -583,31 +383,6 @@ func onOperatorStartLeading(ctx context.Context) {
 		enableCiliumEndpointSyncGC(true)
 	}
 
-	err = enableCNPWatcher()
-	if err != nil {
-		log.WithError(err).WithField(logfields.LogSubsys, "CNPWatcher").Fatal(
-			"Cannot connect to Kubernetes apiserver ")
-	}
-
-	err = enableCCNPWatcher()
-	if err != nil {
-		log.WithError(err).WithField(logfields.LogSubsys, "CCNPWatcher").Fatal(
-			"Cannot connect to Kubernetes apiserver ")
-	}
-
-	if operatorOption.Config.EnableIngressController {
-		ingressController, err := ingress.NewIngressController(
-			ingress.WithHTTPSEnforced(operatorOption.Config.EnforceIngressHTTPS),
-			ingress.WithSecretsSyncEnabled(operatorOption.Config.EnableIngressSecretsSync),
-			ingress.WithSecretsNamespace(operatorOption.Config.IngressSecretsNamespace),
-			ingress.WithLBAnnotationPrefixes(operatorOption.Config.IngressLBAnnotationPrefixes))
-		if err != nil {
-			log.WithError(err).WithField(logfields.LogSubsys, ingress.Subsys).Fatal(
-				"Failed to start ingress controller")
-		}
-		go ingressController.Run()
-	}
-
 	log.Info("Initialization complete")
 
 	<-shutdownSignal
-- 
2.39.1

